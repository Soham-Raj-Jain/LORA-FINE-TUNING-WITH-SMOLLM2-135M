{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "115c7f3fd07044df83371ae3835d0722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_848800ca288345d3b5eef24dea89eb71",
              "IPY_MODEL_e333eda5aca848378f055363a67b6c3d",
              "IPY_MODEL_4b43fd13d8a84a87a3f5352148c8aa95"
            ],
            "layout": "IPY_MODEL_859437fb450e440ca68e6c4e1532fa98"
          }
        },
        "848800ca288345d3b5eef24dea89eb71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57e40d422bfb4273af1398ef1b2033ae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_eac62f7870c3493ab2b6a349cb939c0b",
            "value": "Map:‚Äá100%"
          }
        },
        "e333eda5aca848378f055363a67b6c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c3dfa786ba74377bd17c26cf04e70a4",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9761186bc2b64149abcc1e48af7d2221",
            "value": 500
          }
        },
        "4b43fd13d8a84a87a3f5352148c8aa95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f0a434f1c894574a2f26bc13fae68d4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a05a25e1c54248d6a593b89ffa67d7d6",
            "value": "‚Äá500/500‚Äá[00:00&lt;00:00,‚Äá14962.02‚Äáexamples/s]"
          }
        },
        "859437fb450e440ca68e6c4e1532fa98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57e40d422bfb4273af1398ef1b2033ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac62f7870c3493ab2b6a349cb939c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c3dfa786ba74377bd17c26cf04e70a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9761186bc2b64149abcc1e48af7d2221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f0a434f1c894574a2f26bc13fae68d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a05a25e1c54248d6a593b89ffa67d7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66a73d1a5be14c33a3e282ae3f99ff63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9666a0622cd14b63b5bcc18c094c9e5a",
              "IPY_MODEL_51b471d2b8ac44028441ffe64eb50839",
              "IPY_MODEL_838c6e70bf334d9ca0434486b92a13c1"
            ],
            "layout": "IPY_MODEL_a0afe6bb3ada4bcfb99e82be0d7cc41b"
          }
        },
        "9666a0622cd14b63b5bcc18c094c9e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2abafbef42fb4be587fb0ff3d8c421fa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e97929350cb0405fac3eb27c82868816",
            "value": "Map:‚Äá100%"
          }
        },
        "51b471d2b8ac44028441ffe64eb50839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30947c24afc64516870b4d87ba7445f2",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20aee311a9dc4540a081361d7a58fdac",
            "value": 500
          }
        },
        "838c6e70bf334d9ca0434486b92a13c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2799d489fd2143cf83ac3806b5831be7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0e655023277c4a5eb1ed810e9d4c5838",
            "value": "‚Äá500/500‚Äá[00:00&lt;00:00,‚Äá1758.06‚Äáexamples/s]"
          }
        },
        "a0afe6bb3ada4bcfb99e82be0d7cc41b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2abafbef42fb4be587fb0ff3d8c421fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e97929350cb0405fac3eb27c82868816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30947c24afc64516870b4d87ba7445f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20aee311a9dc4540a081361d7a58fdac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2799d489fd2143cf83ac3806b5831be7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e655023277c4a5eb1ed810e9d4c5838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "115c7f3fd07044df83371ae3835d0722",
            "848800ca288345d3b5eef24dea89eb71",
            "e333eda5aca848378f055363a67b6c3d",
            "4b43fd13d8a84a87a3f5352148c8aa95",
            "859437fb450e440ca68e6c4e1532fa98",
            "57e40d422bfb4273af1398ef1b2033ae",
            "eac62f7870c3493ab2b6a349cb939c0b",
            "8c3dfa786ba74377bd17c26cf04e70a4",
            "9761186bc2b64149abcc1e48af7d2221",
            "7f0a434f1c894574a2f26bc13fae68d4",
            "a05a25e1c54248d6a593b89ffa67d7d6",
            "66a73d1a5be14c33a3e282ae3f99ff63",
            "9666a0622cd14b63b5bcc18c094c9e5a",
            "51b471d2b8ac44028441ffe64eb50839",
            "838c6e70bf334d9ca0434486b92a13c1",
            "a0afe6bb3ada4bcfb99e82be0d7cc41b",
            "2abafbef42fb4be587fb0ff3d8c421fa",
            "e97929350cb0405fac3eb27c82868816",
            "30947c24afc64516870b4d87ba7445f2",
            "20aee311a9dc4540a081361d7a58fdac",
            "2799d489fd2143cf83ac3806b5831be7",
            "0e655023277c4a5eb1ed810e9d4c5838"
          ]
        },
        "id": "gdEQ2tcczWuZ",
        "outputId": "138590ef-9da2-4d00-b29d-af9e8090c8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing Unsloth and dependencies...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ Installation complete!\n",
            "üî• PyTorch: 2.8.0+cu126\n",
            "üéÆ CUDA available: True\n",
            "üéØ GPU: Tesla T4\n",
            "\n",
            "üîß Configuration:\n",
            "   ‚Ä¢ Using LoRA: True\n",
            "   ‚Ä¢ LoRA Rank (r): 16\n",
            "   ‚Ä¢ LoRA Alpha: 16\n",
            "   ‚Ä¢ LoRA Dropout: 0.05\n",
            "   ‚Ä¢ Max Sequence Length: 512\n",
            "   ‚Ä¢ Batch Size: 2\n",
            "   ‚Ä¢ Learning Rate: 0.0002\n",
            "   ‚Ä¢ Max Steps: 50\n",
            "\n",
            "üì• Loading SmolLM2-135M model...\n",
            "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "HuggingFaceTB/SmolLM2-135M-Instruct does not have a padding token! Will use pad_token = <|endoftext|>.\n",
            "‚úÖ Model loaded: HuggingFaceTB/SmolLM2-135M-Instruct\n",
            "üìä Base params: 134.5M\n",
            "üîß Applying LoRA adapters...\n",
            "‚úÖ Trainable: 4.88M / 86.32M (5.66%)\n",
            "üìö Loading dataset (500 examples)...\n",
            "‚úÖ Dataset size: 500\n",
            "üìù Sample:\n",
            "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'input': '', 'instruction': 'Give three tips for staying healthy.'}\n",
            "üß± Formatting to prompt strings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "115c7f3fd07044df83371ae3835d0722"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÇÔ∏è Tokenizing with hard truncation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66a73d1a5be14c33a3e282ae3f99ff63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Max tokenized length (should be ‚â§ 512): 512\n",
            "‚öôÔ∏è Configuring TrainingArguments...\n",
            "üèãÔ∏è Initializing trainer...\n",
            "‚úÖ Trainer ready!\n",
            "üöÄ Starting LoRA fine-tuning...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 4,884,480 of 139,399,488 (3.50% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 01:59, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.152200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.838000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.980900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.843600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.662000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.669700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.462100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.570400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "‚úÖ Training complete!\n",
            "üìä Stats:\n",
            "   ‚Ä¢ Steps: N/A\n",
            "   ‚Ä¢ Train loss: 1.7482541847229003\n",
            "   ‚Ä¢ Time (s): 122.2901\n",
            "üíæ Saving LoRA adapters...\n",
            "‚úÖ Adapters ‚Üí ./smollm2_lora_adapters\n",
            "üîç Size:\n",
            "24M\tsmollm2_lora_adapters\n",
            "\n",
            "üíæ Merging LoRA adapters with base model (optional)...\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `smollm2_lora_merged`: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `smollm2_lora_merged`\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 304.73it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/smollm2_lora_merged`\n",
            "‚úÖ Merged model ‚Üí ./smollm2_lora_merged\n",
            "\n",
            "üß™ Inference test...\n",
            "\n",
            "============================================================\n",
            "INFERENCE RESULTS (LoRA Fine-tuned Model)\n",
            "============================================================\n",
            "\n",
            "[Test 1]\n",
            "Instruction: What is machine learning?\n",
            "Response: Machine learning is a branch of artificial intelligence that focuses on enabling computers to learn from data and improve their performance over time. It involves using algorithms and statistical techniques to analyze and interpret data to identify patterns and relationships that can inform predictions, classify new data, and make informed decisions. Machine learning is used in a variety of applications, including but not limited to:\n",
            "\n",
            "- Predicting stock prices and trading volumes\n",
            "- Analyzing customer behavior and sales data\n",
            "- Identifying spam and phishing attempts\n",
            "- Understanding and classifying complex data sets\n",
            "\n",
            "Machine learning is commonly used in various industries, such as:\n",
            "\n",
            "- Healthcare: Predicting patient outcomes,\n",
            "------------------------------------------------------------\n",
            "\n",
            "[Test 2]\n",
            "Instruction: Write a Python function to calculate Fibonacci numbers.\n",
            "Response: ```\n",
            "def fibonacci(n):\n",
            "    # Implement the function here\n",
            "------------------------------------------------------------\n",
            "\n",
            "[Test 3]\n",
            "Instruction: Explain the water cycle in simple terms.\n",
            "Response: The water cycle is a continuous process that helps keep our planet alive. Here are some of the main stages:\n",
            "1. Water in the Earth is formed in the oceans and in rivers and lakes.\n",
            "2. Water in the oceans heats up in the sun, turning from a liquid to a gas.\n",
            "3. As the water in the oceans heats up, it turns back into water in the oceans.\n",
            "4. Water in the oceans is then carried around the world by rivers and lakes.\n",
            "5. Water in the oceans is then carried around by wind and other weather phenomena.\n",
            "6. Water in the oceans is then carried\n",
            "------------------------------------------------------------\n",
            "\n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë             LORA FINE-TUNING COMPLETE ‚Äî SUMMARY            ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "‚Ä¢ Model: HuggingFaceTB/SmolLM2-135M-Instruct\n",
            "‚Ä¢ LoRA: r=16 over attention & MLP (dropout=0.05)\n",
            "‚Ä¢ Data: 500 Alpaca examples (hard-truncated to 512 tokens)\n",
            "‚Ä¢ Steps: 50 (demo)\n",
            "‚Ä¢ Saved:\n",
            "    - Adapters: ./smollm2_lora_adapters  (tiny files)\n",
            "    - Merged model (optional): ./smollm2_lora_merged\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# COLAB 2: LoRA FINE-TUNING (r=16) FOR SmolLM2-135M WITH UNSLOTH (FIXED)\n",
        "# - Pre-tokenize + hard-truncate to ‚â§512 (avoids CE shape mismatch)\n",
        "# - W&B disabled by default (optional)\n",
        "# - 4-bit friendly optimizer\n",
        "# - Robust merge (supports different Unsloth versions)\n",
        "# =============================================================================\n",
        "\n",
        "# Cell 1: Install\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üì¶ Installing Unsloth and dependencies...\")\n",
        "!pip install -q unsloth\n",
        "!pip install -q --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q datasets trl transformers accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "\n",
        "# Cell 2: Imports & environment\n",
        "# -----------------------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Disable Weights & Biases by default (enable by setting to \"false\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# If TorchDynamo still interferes with Unsloth fused CE, uncomment:\n",
        "# os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
        "\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"üéØ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "\n",
        "# Cell 3: Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "max_seq_length = 512\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# LoRA configuration\n",
        "USE_LORA = True\n",
        "lora_r = 16\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 2\n",
        "gradient_accumulation_steps = 4\n",
        "num_train_epochs = 1               # ignored when max_steps > 0\n",
        "learning_rate = 2e-4               # a bit higher for LoRA\n",
        "max_steps = 50\n",
        "\n",
        "print(f\"\"\"\n",
        "üîß Configuration:\n",
        "   ‚Ä¢ Using LoRA: {USE_LORA}\n",
        "   ‚Ä¢ LoRA Rank (r): {lora_r}\n",
        "   ‚Ä¢ LoRA Alpha: {lora_alpha}\n",
        "   ‚Ä¢ LoRA Dropout: {lora_dropout}\n",
        "   ‚Ä¢ Max Sequence Length: {max_seq_length}\n",
        "   ‚Ä¢ Batch Size: {batch_size}\n",
        "   ‚Ä¢ Learning Rate: {learning_rate}\n",
        "   ‚Ä¢ Max Steps: {max_steps}\n",
        "\"\"\")\n",
        "\n",
        "# Cell 4: Load model & tokenizer\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üì• Loading SmolLM2-135M model...\")\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Ensure PAD/EOS & truncation are explicit\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if getattr(model.config, \"pad_token_id\", None) is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.model_max_length = max_seq_length\n",
        "tokenizer.truncation_side = \"right\"\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model_name}\")\n",
        "try:\n",
        "    print(f\"üìä Base params: {model.num_parameters() / 1e6:.1f}M\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Cell 5: Apply LoRA adapters (attention + MLP only)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üîß Applying LoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_r,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # attention\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",         # MLP\n",
        "    ],\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ Trainable: {trainable_params/1e6:.2f}M / {total_params/1e6:.2f}M \"\n",
        "      f\"({100*trainable_params/total_params:.2f}%)\")\n",
        "\n",
        "# Cell 6: Load dataset (same subset for fair comparison)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üìö Loading dataset (500 examples)...\")\n",
        "raw_ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")\n",
        "print(f\"‚úÖ Dataset size: {len(raw_ds)}\")\n",
        "print(\"üìù Sample:\")\n",
        "print(raw_ds[0])\n",
        "\n",
        "# Cell 7: Build prompts, pre-tokenize, hard-truncate to ‚â§512\n",
        "# -----------------------------------------------------------------------------\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def build_texts(examples):\n",
        "    insts, ins, outs = examples[\"instruction\"], examples[\"input\"], examples[\"output\"]\n",
        "    texts = []\n",
        "    for inst, inp, out in zip(insts, ins, outs):\n",
        "        if inp:\n",
        "            inst = inst + \"\\n\" + inp\n",
        "        texts.append(alpaca_prompt.format(inst, out) + EOS_TOKEN)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"üß± Formatting to prompt strings...\")\n",
        "fmt_ds = raw_ds.map(build_texts, batched=True)\n",
        "\n",
        "def tokenize_and_truncate(examples):\n",
        "    enc = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        padding=False,\n",
        "        add_special_tokens=False,\n",
        "    )\n",
        "    # Labels mirror inputs for causal LM\n",
        "    enc[\"labels\"] = [ids.copy() for ids in enc[\"input_ids\"]]\n",
        "    if \"attention_mask\" not in enc:\n",
        "        enc[\"attention_mask\"] = [[1]*len(ids) for ids in enc[\"input_ids\"]]\n",
        "    return enc\n",
        "\n",
        "print(\"‚úÇÔ∏è Tokenizing with hard truncation...\")\n",
        "tok_ds = fmt_ds.map(\n",
        "    tokenize_and_truncate,\n",
        "    batched=True,\n",
        "    remove_columns=list(fmt_ds.features),   # keep only tokenized fields\n",
        ")\n",
        "\n",
        "max_len = max(len(x) for x in tok_ds[\"input_ids\"])\n",
        "print(\"üîé Max tokenized length (should be ‚â§ 512):\", max_len)\n",
        "\n",
        "# Cell 8: Training arguments\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"‚öôÔ∏è Configuring TrainingArguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_finetuned_smollm2\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    warmup_steps=5,\n",
        "    max_steps=max_steps,                      # precedence over epochs\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=5,\n",
        "    optim=\"adamw_bnb_8bit\",                   # 4-bit friendly\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    report_to=[] if os.environ.get(\"WANDB_DISABLED\",\"true\").lower()==\"true\" else [\"wandb\"],\n",
        ")\n",
        "\n",
        "# Cell 9: Trainer (use tokenized dataset; no dataset_text_field)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üèãÔ∏è Initializing trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tok_ds,\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=False,   # we already tokenized & truncated\n",
        "    args=training_args,\n",
        ")\n",
        "print(\"‚úÖ Trainer ready!\")\n",
        "\n",
        "# Cell 10: Train\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üöÄ Starting LoRA fine-tuning...\")\n",
        "print(\"=\" * 60)\n",
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics or {}\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ Training complete!\")\n",
        "print(\"üìä Stats:\")\n",
        "print(\"   ‚Ä¢ Steps:\", metrics.get(\"train_steps\", metrics.get(\"global_step\", \"N/A\")))\n",
        "print(\"   ‚Ä¢ Train loss:\", metrics.get(\"train_loss\", \"N/A\"))\n",
        "print(\"   ‚Ä¢ Time (s):\", metrics.get(\"train_runtime\", \"N/A\"))\n",
        "\n",
        "# Cell 11: Save LoRA adapters\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üíæ Saving LoRA adapters...\")\n",
        "model.save_pretrained(\"smollm2_lora_adapters\")\n",
        "tokenizer.save_pretrained(\"smollm2_lora_adapters\")\n",
        "print(\"‚úÖ Adapters ‚Üí ./smollm2_lora_adapters\")\n",
        "print(\"üîç Size:\")\n",
        "!du -sh smollm2_lora_adapters\n",
        "\n",
        "# Cell 12: Save merged model (optional; single artifact)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüíæ Merging LoRA adapters with base model (optional)...\")\n",
        "merged_ok = False\n",
        "try:\n",
        "    # Preferred helper (if available in your Unsloth version)\n",
        "    model.save_pretrained_merged(\n",
        "        \"smollm2_lora_merged\",\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\",\n",
        "    )\n",
        "    merged_ok = True\n",
        "except Exception as e:\n",
        "    print(\"   save_pretrained_merged unavailable, trying manual merge:\", repr(e))\n",
        "    try:\n",
        "        FastLanguageModel.merge_lora_weights(model)\n",
        "        model.save_pretrained(\"smollm2_lora_merged\")\n",
        "        tokenizer.save_pretrained(\"smollm2_lora_merged\")\n",
        "        merged_ok = True\n",
        "    except Exception as e2:\n",
        "        print(\"   Manual merge failed (not critical):\", repr(e2))\n",
        "\n",
        "print(\"‚úÖ Merged model ‚Üí ./smollm2_lora_merged\" if merged_ok else \"‚ÑπÔ∏è Skipping merge (adapters are saved and usable).\")\n",
        "\n",
        "# Cell 13: Inference test\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüß™ Inference test...\\n\")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def build_prompt_for_infer(instruction, input_text=\"\"):\n",
        "    text = instruction if not input_text else instruction + \"\\n\" + input_text\n",
        "    return alpaca_prompt.format(text, \"\")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_response(instruction, input_text=\"\"):\n",
        "    prompt = build_prompt_for_infer(instruction, input_text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return text.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INFERENCE RESULTS (LoRA Fine-tuned Model)\")\n",
        "print(\"=\" * 60)\n",
        "tests = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Write a Python function to calculate Fibonacci numbers.\",\n",
        "    \"Explain the water cycle in simple terms.\",\n",
        "]\n",
        "for i, instruction in enumerate(tests, 1):\n",
        "    print(f\"\\n[Test {i}]\")\n",
        "    print(\"Instruction:\", instruction)\n",
        "    print(\"Response:\", generate_response(instruction))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Cell 14: Summary\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë             LORA FINE-TUNING COMPLETE ‚Äî SUMMARY            ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "‚Ä¢ Model: HuggingFaceTB/SmolLM2-135M-Instruct\n",
        "‚Ä¢ LoRA: r=16 over attention & MLP (dropout=0.05)\n",
        "‚Ä¢ Data: 500 Alpaca examples (hard-truncated to 512 tokens)\n",
        "‚Ä¢ Steps: 50 (demo)\n",
        "‚Ä¢ Saved:\n",
        "    - Adapters: ./smollm2_lora_adapters  (tiny files)\n",
        "    - Merged model (optional): ./smollm2_lora_merged\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQpCVtbTz9u0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}